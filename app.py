from flask import Flask, request, jsonify
import re
import pickle
import pandas as pd
import numpy as np
import logging
import os

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
try:
    with open('xgboost_token_model.pkl', 'rb') as f:
        model_artifacts = pickle.load(f)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    model_artifacts = None

def parse_token_data(text):
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    """
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞
        token_data = {}
        
        # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        
        # Market Cap
        mcap_match = re.search(r'MCap:\s*\$?([0-9.]+)([KMB]?)', text, re.IGNORECASE)
        if mcap_match:
            value = float(mcap_match.group(1))
            unit = mcap_match.group(2).upper()
            if unit == 'K':
                value *= 1000
            elif unit == 'M':
                value *= 1000000
            elif unit == 'B':
                value *= 1000000000
            token_data['market_cap'] = value
        else:
            token_data['market_cap'] = 0
        
        # Liquidity
        liq_match = re.search(r'Liq:\s*\$?([0-9.]+)([KMB]?)', text, re.IGNORECASE)
        if liq_match:
            value = float(liq_match.group(1))
            unit = liq_match.group(2).upper()
            if unit == 'K':
                value *= 1000
            elif unit == 'M':
                value *= 1000000
            elif unit == 'B':
                value *= 1000000000
            token_data['liquidity'] = value
        else:
            token_data['liquidity'] = 0
        
        # Volume
        vol_match = re.search(r'Volume\s+\d+min:\s*\$?([0-9.]+)([KMB]?)', text, re.IGNORECASE)
        if vol_match:
            value = float(vol_match.group(1))
            unit = vol_match.group(2).upper()
            if unit == 'K':
                value *= 1000
            elif unit == 'M':
                value *= 1000000
            elif unit == 'B':
                value *= 1000000000
            token_data['volume_1min'] = value
        else:
            token_data['volume_1min'] = 0
        
        # Last Volume
        last_vol_match = re.search(r'Last Volume:\s*\$?([0-9.]+)([KMB]?)\s+([0-9.]+)x', text, re.IGNORECASE)
        if last_vol_match:
            value = float(last_vol_match.group(1))
            unit = last_vol_match.group(2).upper()
            multiplier = float(last_vol_match.group(3))
            
            if unit == 'K':
                value *= 1000
            elif unit == 'M':
                value *= 1000000
            elif unit == 'B':
                value *= 1000000000
                
            token_data['last_volume'] = value
            token_data['last_volume_multiplier'] = multiplier
        else:
            token_data['last_volume'] = 0
            token_data['last_volume_multiplier'] = 1
        
        # Token Age (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–∏–Ω—É—Ç—ã)
        age_match = re.search(r'Token Age:\s*(?:(\d+)h\s*)?(?:(\d+)m\s*)?(?:(\d+)s\s*)?', text, re.IGNORECASE)
        if age_match:
            hours = int(age_match.group(1)) if age_match.group(1) else 0
            minutes = int(age_match.group(2)) if age_match.group(2) else 0
            seconds = int(age_match.group(3)) if age_match.group(3) else 0
            total_minutes = hours * 60 + minutes + seconds / 60
            token_data['token_age_numeric'] = total_minutes
        else:
            token_data['token_age_numeric'] = 0
        
        # –ü–∞—Ä—Å–∏–º –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π –ø–æ —Ü–≤–µ—Ç–∞–º
        holders_match = re.search(r'üü¢:\s*(\d+)\s*\|\s*üîµ:\s*(\d+)\s*\|\s*üü°:\s*(\d+)\s*\|\s*‚≠ïÔ∏è:\s*(\d+)', text)
        if holders_match:
            token_data['green_holders'] = int(holders_match.group(1))
            token_data['blue_holders'] = int(holders_match.group(2))
            token_data['yellow_holders'] = int(holders_match.group(3))
            token_data['circle_holders'] = int(holders_match.group(4))
        else:
            token_data['green_holders'] = 0
            token_data['blue_holders'] = 0
            token_data['yellow_holders'] = 0
            token_data['circle_holders'] = 0
        
        # –ü–∞—Ä—Å–∏–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π
        special_holders_match = re.search(r'ü§°:\s*(\d+)\s*\|\s*üåû:\s*(\d+)\s*\|\s*üåó:\s*(\d+)\s*\|\s*üåö:\s*(\d+)', text)
        if special_holders_match:
            token_data['clown_holders'] = int(special_holders_match.group(1))
            token_data['sun_holders'] = int(special_holders_match.group(2))
            token_data['half_moon_holders'] = int(special_holders_match.group(3))
            token_data['dark_moon_holders'] = int(special_holders_match.group(4))
        else:
            token_data['clown_holders'] = 0
            token_data['sun_holders'] = 0
            token_data['half_moon_holders'] = 0
            token_data['dark_moon_holders'] = 0
        
        # Total holders
        total_holders_match = re.search(r'Total:\s*(\d+)', text)
        if total_holders_match:
            token_data['total_holders'] = int(total_holders_match.group(1))
        else:
            # –°—á–∏—Ç–∞–µ–º –∫–∞–∫ —Å—É–º–º—É –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π
            token_data['total_holders'] = (
                token_data['green_holders'] + token_data['blue_holders'] + 
                token_data['yellow_holders'] + token_data['circle_holders']
            )
        
        # Top 10 percent
        top10_match = re.search(r'Top10:\s*([0-9.]+)%', text)
        if top10_match:
            token_data['top10_percent'] = float(top10_match.group(1))
        else:
            token_data['top10_percent'] = 50.0  # default
        
        # Total percent –∏ Total now percent
        total_match = re.search(r'Total:\s*([0-9.]+)%', text)
        if total_match:
            token_data['total_percent'] = float(total_match.group(1))
        else:
            token_data['total_percent'] = 100.0
        
        total_now_match = re.search(r'Total now:\s*([0-9.]+)%', text)
        if total_now_match:
            token_data['total_now_percent'] = float(total_now_match.group(1))
        else:
            token_data['total_now_percent'] = 100.0
        
        # Insiders
        insiders_match = re.search(r'Insiders:\s*(\d+)\s+hold\s+([0-9.]+)%', text)
        if insiders_match:
            token_data['insiders_count'] = int(insiders_match.group(1))
            token_data['insiders_percent'] = float(insiders_match.group(2))
        else:
            token_data['insiders_count'] = 0
            token_data['insiders_percent'] = 0.0
        
        # Snipers
        snipers_match = re.search(r'Snipers:\s*(\d+)', text)
        if snipers_match:
            token_data['snipers_count'] = int(snipers_match.group(1))
        else:
            token_data['snipers_count'] = 0
        
        # Bundle
        bundle_total_match = re.search(r'Bundle:.*?Total:\s*(\d+)', text, re.DOTALL)
        if bundle_total_match:
            token_data['bundle_total'] = int(bundle_total_match.group(1))
        else:
            token_data['bundle_total'] = 0
        
        bundle_supply_match = re.search(r'Supply:\s*([0-9.]+)%', text)
        if bundle_supply_match:
            token_data['bundle_supply_percent'] = float(bundle_supply_match.group(1))
        else:
            token_data['bundle_supply_percent'] = 0.0
        
        # Dev holds
        dev_holds_match = re.search(r'Dev holds\s+([0-9.]+)%', text)
        if dev_holds_match:
            token_data['dev_holds_percent'] = float(dev_holds_match.group(1))
        else:
            token_data['dev_holds_percent'] = 0.0
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞–∫ –≤ –º–æ–¥–µ–ª–∏)
        token_data['volume_to_liquidity'] = float(
            np.log1p(token_data['volume_1min']) / np.log1p(token_data['liquidity'] + 1)
            if token_data['liquidity'] > 0 else 0
        )
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        token_data['log_market_cap'] = float(np.log1p(token_data['market_cap']))
        token_data['log_liquidity'] = float(np.log1p(token_data['liquidity']))
        token_data['log_volume_1min'] = float(np.log1p(token_data['volume_1min']))
        token_data['log_last_volume'] = float(np.log1p(token_data['last_volume']))
        
        # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π
        token_data['holder_concentration'] = int(
            token_data['green_holders'] + token_data['blue_holders'] + 
            token_data['yellow_holders'] + token_data['circle_holders']
        )
        
        # –†–∏—Å–∫-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        token_data['total_risk_percent'] = float(
            token_data['dev_holds_percent'] + token_data['insiders_percent']
        )
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ —Ç–∏–ø—ã
        return convert_to_json_serializable(token_data)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return {}

def convert_to_json_serializable(obj):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    else:
        return obj

def predict_token_success(token_data):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞ (—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö)
    """
    if model_artifacts is None:
        return {'error': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}
    
    try:
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df_new = pd.DataFrame([token_data])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
        for col in model_artifacts['feature_names']:
            if col not in df_new.columns:
                df_new[col] = 0
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        df_new = df_new[model_artifacts['feature_names']]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–º–ø—É—Ç–µ—Ä
        df_imputed = pd.DataFrame(
            model_artifacts['imputer'].transform(df_new), 
            columns=model_artifacts['feature_names']
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        raw_prediction = int(model_artifacts['model'].predict(df_imputed)[0])
        raw_probability = float(model_artifacts['model'].predict_proba(df_imputed)[0, 1])
        
        # üîÑ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –î–†–ò–§–¢–ê: –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
        # –ü–æ—Å–∫–æ–ª—å–∫—É –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–æ–±–æ—Ä–æ—Ç –∏–∑-–∑–∞ –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
        probability = 1.0 - raw_probability
        prediction = int(probability >= 0.5)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_score = abs(probability - 0.5) * 2
        if confidence_score > 0.8:
            confidence_level = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
        elif confidence_score > 0.6:
            confidence_level = "–í—ã—Å–æ–∫–∞—è"
        elif confidence_score > 0.4:
            confidence_level = "–°—Ä–µ–¥–Ω—è—è"
        else:
            confidence_level = "–ù–∏–∑–∫–∞—è"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        if probability >= 0.7:
            recommendation = "–ü–û–ö–£–ü–ê–¢–¨"
            risk_level = "–ù–∏–∑–∫–∏–π"
        elif probability >= 0.5:
            recommendation = "–ò–ó–£–ß–ò–¢–¨"
            risk_level = "–°—Ä–µ–¥–Ω–∏–π"
        else:
            recommendation = "–ü–†–û–ü–£–°–¢–ò–¢–¨"
            risk_level = "–í—ã—Å–æ–∫–∏–π"
        
        result = {
            'prediction': '–î–ê' if prediction == 1 else '–ù–ï–¢',
            'probability': round(float(probability), 4),
            'probability_percent': f"{probability*100:.1f}%",
            'confidence_level': confidence_level,
            'confidence_score': round(float(confidence_score), 4),
            'recommendation': recommendation,
            'risk_level': risk_level,
            'threshold_conservative': '–î–ê' if probability >= 0.7 else '–ù–ï–¢',
            'threshold_optimal': '–î–ê' if probability >= 0.5 else '–ù–ï–¢',
            'threshold_aggressive': '–î–ê' if probability >= 0.3 else '–ù–ï–¢',
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            'drift_correction': {
                'raw_probability': round(raw_probability, 4),
                'corrected_probability': round(probability, 4),
                'inversion_applied': True,
                'note': '–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö'
            }
        }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ —Ç–∏–ø—ã
        return convert_to_json_serializable(result)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        return {'error': f'–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}'}

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model_artifacts is not None,
        'version': '2.0',
        'drift_correction': 'enabled'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–ª–µ "text" —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞'}), 400
        
        # –ü–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç
        token_data = parse_token_data(data['text'])
        
        if not token_data:
            return jsonify({'error': '–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∞'}), 400
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        result = predict_token_success(token_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if data.get('include_parsed_data', False):
            result['parsed_data'] = convert_to_json_serializable(token_data)
        
        return jsonify(result)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ API: {e}")
        return jsonify({'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}'}), 500

@app.route('/test', methods=['GET'])
def test():
    """–¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —Å –ø—Ä–∏–º–µ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö"""
    test_text = """üçÄ Maybe Scalp it? ‚úØ‚úØ‚úØ‚úØ‚úØ

üé≤ $lazycoin | does nothing

8erVzAoR4yD22fmQYB1kUGn9uLQuDaLZjd85wnhWbonk | ùïès
‚îî Launchpad: LetsBonk.fun

‚è≥Token Age: 58m 32s
 ‚îú MCap: $88K
 ‚îî Liq: $40K
 ‚îî Volume 5min: $73k üü°
 ‚îî Last Volume: $4k 14.9x üü£
> First 70 holders:


üåöüåöüåöüåöüåö‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüü¢‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
üü°üü°‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüîµüü°
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüü°‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è

‚îú üü¢: 1 | üîµ: 1 | üü°: 4 | ‚≠ïÔ∏è: 59
‚îú ü§°: 0 | üåû: 0 | üåó: 0 | üåö: 5
‚îú Total: 87.5%
‚îî Total now: 1.1%
> Holders:
 ‚îú Top10: 23.7% üü¢| Total: 406
 ‚îú Insiders: 0 hold 0% üü¢
 ‚îî Snipers: 5
> Bundle:
 ‚îú Total: 0
 ‚îî Supply: 0% üü¢
> Dev:
 ‚îú Migrations: 2/319 (0.6%)
 ‚îî Dev holds 0% üü¢"""
    
    token_data = parse_token_data(test_text)
    result = predict_token_success(token_data)
    result['parsed_data'] = convert_to_json_serializable(token_data)
    
    return jsonify(result)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))  # Railway –ø–µ—Ä–µ–¥–∞–µ—Ç –ø–æ—Ä—Ç —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Token Prediction API v2.0 –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    print(f"üìä –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: –í–ö–õ–Æ–ß–ï–ù–ê")
    print(f"üß™ –¢–µ—Å—Ç: http://localhost:{port}/test")
    print(f"‚ù§Ô∏è  –°—Ç–∞—Ç—É—Å: http://localhost:{port}/health")
    app.run(host='0.0.0.0', port=port, debug=False)
